{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52880eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "022075dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "embed_dim = 4\n",
    "sequence_length = len(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9db05a",
   "metadata": {},
   "source": [
    "# Sinusoidal Positional Encoding\n",
    "We want to find a way to represent the position of words in a setence. We can use the following equations to get positional encodings (PE):\n",
    "\n",
    "$PE(pos, 2i) = sin(\\frac{pos}{10000 ^ {2i / d_{\\text{model}}}})$ \n",
    "\n",
    "\n",
    "$PE(pos, 2i+1) = cos(\\frac{pos}{10000 ^ {2i / d_{\\text{model}}}})$\n",
    "\n",
    "\n",
    "Here, pos is the position of the word in the sentence, d_model is the dimension of the embedding vector. i gives us the index of the dimension in the embedding vector.\n",
    "\n",
    "\n",
    "Notice here that there are two indices for i in the equation. 2i and 2i + 1. This means for each i, we have two dimensions in the embedding vector. One dimension uses the sine function (2i), and the other dimension uses the cosine function (2i + 1).\n",
    "\n",
    "\n",
    "For example, if the embedding dimension is 16,  then for word at position 0, 2, 4, 6, 8, 10, 12, 14, we will use the sine function, and for position 1, 3, 5, 7, 9, 11, 13, 15, we will use the cosine function. These are given by i = 0, 1, 2, 3, 4, 5, 6, 7 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9f7101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for a is: [0. 1. 0. 1.]\n",
      "Encoding for b is: [0.84147098 0.54030231 0.00999983 0.99995   ]\n",
      "Encoding for c is: [ 0.90929743 -0.41614684  0.01999867  0.99980001]\n",
      "Encoding for d is: [ 0.14112001 -0.9899925   0.0299955   0.99955003]\n",
      "Encoding for e is: [-0.7568025  -0.65364362  0.03998933  0.99920011]\n",
      "Encoding for f is: [-0.95892427  0.28366219  0.04997917  0.99875026]\n",
      "Encoding for g is: [-0.2794155   0.96017029  0.05996401  0.99820054]\n",
      "Encoding for h is: [0.6569866  0.75390225 0.06994285 0.997551  ]\n",
      "Encoding for i is: [ 0.98935825 -0.14550003  0.07991469  0.99680171]\n",
      "Encoding for j is: [ 0.41211849 -0.91113026  0.08987855  0.99595273]\n",
      "Encoding for k is: [-0.54402111 -0.83907153  0.09983342  0.99500417]\n",
      "Encoding for l is: [-0.99999021  0.0044257   0.1097783   0.9939561 ]\n",
      "Encoding for m is: [-0.53657292  0.84385396  0.11971221  0.99280864]\n",
      "Encoding for n is: [0.42016704 0.90744678 0.12963414 0.99156189]\n",
      "Encoding for o is: [0.99060736 0.13673722 0.13954311 0.990216  ]\n",
      "Encoding for p is: [ 0.65028784 -0.75968791  0.14943813  0.98877108]\n",
      "Encoding for q is: [-0.28790332 -0.95765948  0.15931821  0.98722728]\n",
      "Encoding for r is: [-0.96139749 -0.27516334  0.16918235  0.98558477]\n",
      "Encoding for s is: [-0.75098725  0.66031671  0.17902957  0.98384369]\n",
      "Encoding for t is: [0.14987721 0.98870462 0.18885889 0.98200424]\n",
      "Encoding for u is: [0.91294525 0.40808206 0.19866933 0.98006658]\n",
      "Encoding for v is: [ 0.83665564 -0.54772926  0.2084599   0.97803091]\n",
      "Encoding for w is: [-0.00885131 -0.99996083  0.21822962  0.97589745]\n",
      "Encoding for x is: [-0.8462204  -0.53283302  0.22797752  0.9736664 ]\n",
      "Encoding for y is: [-0.90557836  0.42417901  0.23770263  0.97133797]\n",
      "Encoding for z is: [-0.13235175  0.99120281  0.24740396  0.96891242]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def positional_encoding(pos, d_model):\n",
    "    PE = np.zeros(embed_dim) # for each characters, we have 16 digits to represent that character\n",
    "    for i in range(d_model // 2):\n",
    "        denominator = 10000 ** (2 * i / d_model)\n",
    "        PE[2 * i] = np.sin(pos / denominator)\n",
    "        PE[2 * i + 1] = np.cos(pos / denominator)\n",
    "    return PE\n",
    "\n",
    "for pos in range(sequence_length):\n",
    "    print(f'Encoding for {alphabet[pos]} is: {positional_encoding(pos, embed_dim)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99c1bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "        [-0.7568, -0.6536,  0.0400,  0.9992],\n",
      "        [-0.9589,  0.2837,  0.0500,  0.9988],\n",
      "        [-0.2794,  0.9602,  0.0600,  0.9982],\n",
      "        [ 0.6570,  0.7539,  0.0699,  0.9976],\n",
      "        [ 0.9894, -0.1455,  0.0799,  0.9968],\n",
      "        [ 0.4121, -0.9111,  0.0899,  0.9960],\n",
      "        [-0.5440, -0.8391,  0.0998,  0.9950],\n",
      "        [-1.0000,  0.0044,  0.1098,  0.9940],\n",
      "        [-0.5366,  0.8439,  0.1197,  0.9928],\n",
      "        [ 0.4202,  0.9074,  0.1296,  0.9916],\n",
      "        [ 0.9906,  0.1367,  0.1395,  0.9902],\n",
      "        [ 0.6503, -0.7597,  0.1494,  0.9888],\n",
      "        [-0.2879, -0.9577,  0.1593,  0.9872],\n",
      "        [-0.9614, -0.2752,  0.1692,  0.9856],\n",
      "        [-0.7510,  0.6603,  0.1790,  0.9838],\n",
      "        [ 0.1499,  0.9887,  0.1889,  0.9820],\n",
      "        [ 0.9129,  0.4081,  0.1987,  0.9801],\n",
      "        [ 0.8367, -0.5477,  0.2085,  0.9780],\n",
      "        [-0.0089, -1.0000,  0.2182,  0.9759],\n",
      "        [-0.8462, -0.5328,  0.2280,  0.9737],\n",
      "        [-0.9056,  0.4242,  0.2377,  0.9713],\n",
      "        [-0.1324,  0.9912,  0.2474,  0.9689]])\n"
     ]
    }
   ],
   "source": [
    "# pos_encoding = torch.zeros(sequence_length, embed_dim)\n",
    "# position = torch.arange(sequence_length).reshape(-1, 1)\n",
    "# div_term = torch.exp(torch.arange(0, embed_dim, 2) * - torch.log(torch.tensor(10000.0)) / embed_dim)\n",
    "# pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "# pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "# pos_encoding\n",
    "\n",
    "def get_positional_encoding(sequence_length, d_model):\n",
    "    pos_encoding = torch.zeros(sequence_length, d_model)\n",
    "    position = torch.arange(sequence_length).reshape(-1, 1)\n",
    "    div = torch.exp(torch.arange(0, d_model, 2) * -1 * torch.log(torch.tensor(10000.0)) / d_model)\n",
    "    pos_encoding[:, 0::2] = torch.sin(position * div)\n",
    "    pos_encoding[:, 1::2] = torch.cos(position * div)\n",
    "    return pos_encoding\n",
    "\n",
    "print(get_positional_encoding(sequence_length, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f453b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937beb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
