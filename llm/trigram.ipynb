{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e154852",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/names.txt', 'r') as f:\n",
    "    data = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef61d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(''.join([d for d in data])))\n",
    "chars = ['.'] + chars\n",
    "ctoi = {c:i for i, c in enumerate(chars)}\n",
    "itoc = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "print(ctoi)\n",
    "print(itoc)\n",
    "X = []\n",
    "block_size = 2 # Tri-gram\n",
    "for word in data:\n",
    "    context = [0] * block_size\n",
    "    for c in word + '.':\n",
    "        ix = ctoi[c]\n",
    "        X.append(context + [ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ca03c",
   "metadata": {},
   "source": [
    "## Counting the occurence and then predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's 'train' a simple 'network': Count how many times one character follows another and\n",
    "# use that to sample next word to generate word. NOT using a Neural Network\n",
    "\n",
    "\n",
    "counts = torch.zeros(27, 27, 27)\n",
    "# print(counts.size())\n",
    "# counts[0, 0, 0] represents count of '...' sequence in our training data, counts[0, 1, 0] represents count of '.a.' in our training data and so on\n",
    "for val in X:\n",
    "    counts[val[0], val[1], val[2]] += 1\n",
    "\n",
    "\n",
    "print(counts[0, 0, :])\n",
    "# Now Normalize\n",
    "# '...', '..a', '..b', '..c', '..d', ..... and so on should sum to 1.0 (i.e., normalize across count[0, 0, i])\n",
    "# '.a.', '.aa', '.ab', '.ac', '.ad', ... and so on should sum to 1.0 (i.e., across count[0, 1, i])\n",
    "# '.b.', '.ba', '.bb', '.bc', '.bd', ... and so on should sum to 1.0 (i.e., across count[0, 2, i])\n",
    "# ...\n",
    "# 'z..', 'z.a', 'z.b', 'z.c', 'z.d', ...\n",
    "# ...\n",
    "# 'zz.', 'zza', 'zzb', 'zzc', 'zzd', ...\n",
    "\n",
    "# i.e., for every previous occurrences, we have to sum along dimension = 2 (position of i above)\n",
    "\n",
    "counts = counts + 1 # Avoid division by zero (smoothing) as this 1 increase to higher number, we get uniform distribution. Hence smoothing.\n",
    "counts = counts / counts.sum(dim=2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63034c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it sums to 1.0\n",
    "print(sum(counts[0, 0, :]))\n",
    "print(sum(counts[0, 1, :]))\n",
    "print(sum(counts[0, 1, :]))\n",
    "\n",
    "# Rough code to see dimension\n",
    "# ten = torch.tensor([[[1,2,3],[1,2,3],[1,2,3]], [[10,20,30],[10,20,30],[10,20,30]], [[100,200,300],[100,200,300],[100,200,300]]])\n",
    "# print(ten)\n",
    "\n",
    "# print(\"Sum\")\n",
    "# ten.sum(dim=2, keepdim=True)\n",
    "# dim0 = (3, 3)\n",
    "# dim1 = (1, 3)\n",
    "# dim2 = (3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sample from that counts (PREDICTION Step)\n",
    "\n",
    "g = torch.Generator().manual_seed(1024)\n",
    "for _ in range(50):\n",
    "    out = []\n",
    "    ix, prev_ix = 0, 0\n",
    "    while True:\n",
    "        # Now sample index from count[0, 0, :] row.\n",
    "        p = counts[prev_ix, ix, :]\n",
    "        new_ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itoc[new_ix])\n",
    "        if new_ix == 0:\n",
    "            break\n",
    "        prev_ix = ix\n",
    "        ix = new_ix\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the quality of this model:\n",
    "\n",
    "# For that we look at the probability of the dataset from our 'learned' counts model.\n",
    "for x in X[:10]:\n",
    "    print(''.join(itoc[i] for i in x), end='--> ')\n",
    "    print(f'{counts[x[0], x[1], x[2]].item():.2f}')\n",
    "\n",
    "# MLE says that a good model maximizes the product of likelihood of this data\n",
    "# since product of probabilities would be very small, we're taking log probability\n",
    "\n",
    "print(\"Quality of the model: summarized by negative log likelihood\")\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for x in X:\n",
    "    prob = counts[x[0], x[1], x[2]]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    n += 1\n",
    "\n",
    "# Log likelihood = 0 when PS = 1.0 (which we aim to go towards)\n",
    "# Log likelihood = -ve when PS < 1.0 (which says that we haven't fitted the prediction for the likelihood.)\n",
    "\n",
    "# We want a loss function (i.e., 0 when prediction is good, high when prediction is bad)\n",
    "# so we use Negative log likelihood (nll) as loss function.\n",
    "# we also average it instead of sum.\n",
    "nll = -1 * log_likelihood\n",
    "loss = nll/n\n",
    "\n",
    "print(f'{loss=}')\n",
    "\n",
    "# Right now, since we counted how many times 3rd character occurs given first two characters, our model right now is 'perfect'. So this is the loss that we can get to even when we train a neural network to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329d479",
   "metadata": {},
   "source": [
    "## Training a Neural Network for Trigram Character Level Language Model.\n",
    "\n",
    "- We took an approach that we felt natural, i.e, counting the times last character appeared given two characters ahead, we computed the probabilities. We then used that distribution to predict the next character in prediction phase.\n",
    "- Now, as the number of character increase, this becomes computationally infeasible.\n",
    "- So, we want to use a neural network to do that.\n",
    "- Here, we input two characters to the neural network and it predicts the probability distribution over the next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6110ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print input data\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42265652",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for data in X[:5]:\n",
    "    x.append([data[0], data[1]])\n",
    "    y.append(data[2])\n",
    "\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we can't pass int64 (long) into a neural network, we use one hot vector\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1024)\n",
    "\n",
    "x_enc = F.one_hot(x, num_classes=len(chars)).float()\n",
    "print(x_enc.shape)\n",
    "# Reshape it to stack the inputs\n",
    "# X_temp = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[10, 20, 30], [40, 50, 60]]]).float()\n",
    "\n",
    "# print(X_temp.view(len(X_temp), -1))\n",
    "# print(X_temp.reshape(2, 6))\n",
    "\n",
    "x_enc = x_enc.view(len(x_enc), -1)\n",
    "print(f'{x_enc.shape=}')\n",
    "\n",
    "# Now we make a 27 layer output to predict probability distribution over characters\n",
    "# weight = 54 (inputs) * 27 (output layer)\n",
    "\n",
    "W = torch.randn(54, 27)\n",
    "\n",
    "out = x_enc @ W\n",
    "print(f'output shape: {out.shape}')\n",
    "\n",
    "# 1st neuron output for 1st data\n",
    "neuron_output = torch.dot(x_enc[0, :], W[:, 0])\n",
    "print(neuron_output)\n",
    "\n",
    "\n",
    "counts = out.exp()\n",
    "prob_nn = counts / counts.sum(dim=1, keepdim=True)\n",
    "print(f'{prob_nn.shape=}')\n",
    "print(y)\n",
    "# These give the probability for the current target values (true)\n",
    "print(prob_nn[0, 5], prob_nn[1, 13], prob_nn[2, 13], prob_nn[3, 1], prob_nn[4, 0])\n",
    "print(prob_nn[torch.arange(len(prob_nn)), y])\n",
    "\n",
    "# Now to compute logprob\n",
    "logprob = prob_nn[torch.arange(len(prob_nn)), y].log()\n",
    "print(f'{logprob=}')\n",
    "\n",
    "# We can see that logprob of fair predictions (i.e., for 4th data -> 0.1475) is more compared to others (very less prob -> bad predictions). These are true label, so the prob should be higher.\n",
    "\n",
    "# To compute loss, we take negative logprob (and average it)\n",
    "loss = -1 * logprob.mean()\n",
    "print(f'{loss=}')\n",
    "\n",
    "# This loss is pretty high compared to what we got from counting model above. our aim is to get close to counting model loss, because that reflects true probability distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1024)\n",
    "# Now do for all data\n",
    "x = []\n",
    "y = []\n",
    "for data in X:\n",
    "    x.append([data[0], data[1]])\n",
    "    y.append(data[2])\n",
    "\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "# divide into train, validation and test sets\n",
    "n1 = int(0.8 * len(x))\n",
    "n2 = int(0.9 * len(x))\n",
    "x_train, x_val, x_test = x[:n1], x[n1:n2], x[n2:]\n",
    "y_train, y_val, y_test = y[:n1], y[n1:n2], y[n2:]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "# Create a random embedding\n",
    "n_embed = 10\n",
    "C = torch.randn((len(chars), n_embed), requires_grad=True)\n",
    "# x_enc = F.one_hot(x, num_classes=len(chars)).float()\n",
    "# x_enc = x_enc.view(len(x_enc), -1)\n",
    "W = torch.randn((n_embed * 2, 27), requires_grad=True)\n",
    "\n",
    "# print(f\"Num training data: {len(x_enc)}\")\n",
    "# Now we can predict the probability distribution over the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b440fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_pass(x=x_train, y=y_train):\n",
    "    # 1) FORWARD PASS\n",
    "    x_enc = C[x].view(len(x), -1)\n",
    "    logits = x_enc @ W\n",
    "    counts_nn = logits.exp()\n",
    "    prob_nn = counts_nn / counts_nn.sum(dim=1, keepdim=True)\n",
    "    # lines 3, and 4 is softmax\n",
    "    loss = -1 * torch.log(prob_nn[torch.arange(len(prob_nn)), y]).mean()\n",
    "    # Loss is negative logprobs (average)\n",
    "    # Line 4, 5, 7 are simply achievable through F.cross_entropy(logits, y)\n",
    "    # print(F.cross_entropy(logits, y).item()) # This is clled cross_entropy because\n",
    "    print(loss.item())\n",
    "\n",
    "    # 2) Backward Pass\n",
    "    W.grad = None # Clear out gradients\n",
    "    C.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize weights\n",
    "    W.data += -0.1 * W.grad # Gradients point towards the increase in loss.\n",
    "    C.data += -0.1 * C.grad\n",
    "\n",
    "forward_backward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train for couple of epochs\n",
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "    print(f'Epoch {i}:', end=\" \")\n",
    "    forward_backward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "generator = torch.Generator().manual_seed(1024)\n",
    "for _ in range(10):\n",
    "    input_x = torch.tensor([0, 0])\n",
    "    ix = 0\n",
    "    output = []\n",
    "    input_x = torch.cat([input_x[1:],torch.tensor([ix])])\n",
    "    while True:\n",
    "        x_enc = C[input_x].view(1, -1)\n",
    "        logits_pred = x_enc @ W\n",
    "        counts_pred = logits_pred.exp()\n",
    "        probs_pred = counts_pred/counts_pred.sum(dim=1, keepdim=True)\n",
    "        ix = torch.multinomial(probs_pred, num_samples=1, replacement=True, generator=generator).item()\n",
    "        char = itoc[ix]\n",
    "        output.append(char)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5512d49d",
   "metadata": {},
   "source": [
    "Horrible performance.\n",
    "# Now we will make a slightly deeper model and see how that performs\n",
    "For that, we have to organize how we define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we make a linear neuron so that scaling up is easier.\n",
    "# This initializes weights and bias and the output is similar of linear layer.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class Embedding():\n",
    "    def __init__(self, vocab_size, n_embed):\n",
    "        self.C = torch.randn((vocab_size, n_embed)) # Embedding matrix\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.C[x]\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.C]\n",
    "\n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.W = torch.randn(fan_in, fan_out)  *  5/3 / fan_in**0.5 # Kaiming init\n",
    "        self.b = torch.randn(1, fan_out) * 0.01 # Multiply with a small number\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, X):\n",
    "\n",
    "        self.out = X @ self.W\n",
    "        if self.bias is not None:\n",
    "            self.out += self.b\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W] + ([self.b] if self.bias else [])\n",
    "\n",
    "class BatchNorm():\n",
    "    def __init__(self, n_hidden, eps=1e-5, momentum=0.1):\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.training = True\n",
    "\n",
    "        self.bngain = torch.ones(1, n_hidden)\n",
    "        self.bnbias = torch.zeros(1, n_hidden)\n",
    "        self.bnmean_running = torch.zeros(1, n_hidden)\n",
    "        self.bnstd_running = torch.ones(1, n_hidden)\n",
    "\n",
    "    def __call__(self, logits):\n",
    "        if self.training:\n",
    "            mean = logits.mean(dim=0, keepdim=True)\n",
    "            std = logits.std(dim=0, keepdim=True)\n",
    "        else:\n",
    "            mean = self.bnmean_running\n",
    "            std = self.bnstd_running\n",
    "        xhat = (logits - mean) / (std + self.eps)\n",
    "        self.out = self.bngain * xhat + self.bnbias\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.bnmean_running = (1 - self.momentum) * self.bnmean_running + self.momentum * mean\n",
    "                self.bnstd_running = (1 - self.momentum) * self.bnstd_running + self.momentum * std\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.bngain, self.bnbias]\n",
    "\n",
    "\n",
    "class Tanh(object):\n",
    "    def __call__(self, input):\n",
    "        self.out = torch.tanh(input)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "def train(X, y, layers, parameters, epochs, C, lr=0.1, verbose=False):\n",
    "    all_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        # lets take a mini batch of input to do sgd so that it trains fast\n",
    "        ix = torch.randint(0, X.shape[0], (BATCH_SIZE, ))\n",
    "        X_batch = X[ix]\n",
    "        logits = C(X_batch).view(BATCH_SIZE, -1)\n",
    "        # logits = C(X).view(len(X), -1)\n",
    "        # Forward pass\n",
    "        for i, layer in enumerate(layers):\n",
    "            logits = layer(logits)\n",
    "        # Since taking exp of logits, and then dividing it by sum is softmax, we're just going to take softmax\n",
    "        # you can directly compute loss with F.cross_entropy(), that does softmax inside of it\n",
    "        # loss = F.cross_entropy(logits, y)\n",
    "        loss = F.cross_entropy(logits, y[ix])\n",
    "\n",
    "        # backward pass\n",
    "        for layer in layers:\n",
    "            layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "\n",
    "        all_loss.append(loss.log10().item())\n",
    "        # make sure you flush all the gradients\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        # Do backward propagation\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "        if verbose:\n",
    "            print(f'Epoch: {epoch}, loss: {loss.item()}')\n",
    "    return all_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e702fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2,3],\n",
    "                  [3,4,5],\n",
    "                  [6,7,8]])\n",
    "print(a.shape)\n",
    "mean_a_dim_0 = a.mean(dim=0, keepdim=True)\n",
    "print(mean_a_dim_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69517589",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Linear(n_embed * block_size, 27)\n",
    "out = layer(x_enc)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9410335",
   "metadata": {},
   "source": [
    "### Now, we can do the same above steps with a slightly deeper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f891e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 10\n",
    "vocab_size = len(chars)\n",
    "C = Embedding(vocab_size=vocab_size, n_embed=n_embed)\n",
    "layers = [\n",
    "    Linear(n_embed * block_size, 100),\n",
    "    Linear(100, 100),\n",
    "    Linear(100, vocab_size)\n",
    "]\n",
    "parameters = C.parameters() + [p for layer in layers for p in layer.parameters()]\n",
    "print(\"Total parameters = \", sum(p.nelement() for p in parameters))\n",
    "\n",
    "# lets make sure that the use grad is true for all parameters.\n",
    "for p in parameters:\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "all_loss = train(x_train, y_train, layers, parameters, 100000, C, lr=0.10)\n",
    "print(f'Average 10 loss: {sum(all_loss[-10:])/10:.2f}')\n",
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fce18a",
   "metadata": {},
   "source": [
    "#### Didn't improve performance because no matter how much linear layer you stack (wx + b), you can make a single linear layer that is equivalent to the stacked layers.\n",
    "\n",
    "## Let's add non-linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets add a non-linear layer (lets do tanh)\n",
    "C = Embedding(vocab_size=vocab_size, n_embed=n_embed)\n",
    "layers_w_nl = [\n",
    "    Linear(n_embed * block_size, 100), Tanh(),\n",
    "    Linear(100, 100), Tanh(),\n",
    "    Linear(100, 27)\n",
    "]\n",
    "parameters_nl = C.parameters() + [p for layer in layers_w_nl for p in layer.parameters()]\n",
    "print(\"Total parameters = \", sum(p.nelement() for p in parameters))\n",
    "\n",
    "# lets make sure that the use grad is true for all parameters.\n",
    "for p in parameters_nl:\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "# Now train like before\n",
    "all_loss = train(x_train, y_train, layers_w_nl, parameters_nl, 100000, C, lr=0.1)\n",
    "print(f'Average last 10 loss: {sum(all_loss[-10:])/10:.2f}')\n",
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00051c7",
   "metadata": {},
   "source": [
    "## Not a good performance, the loss is very high in the beginning (remember loss plot is in log scale).\n",
    "#### Lets scale the loss down at the initialization (scale down W and b), and increase the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f075419",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Embedding(vocab_size=vocab_size, n_embed=n_embed)\n",
    "layers_w_nl = [\n",
    "    Linear(n_embed * block_size, 100, bias=False), Tanh(),\n",
    "    Linear(100, 100, bias=False), Tanh(),\n",
    "    Linear(100, 100, bias=False), Tanh(),\n",
    "    Linear(100, 100, bias=False), Tanh(),\n",
    "    Linear(100, 100, bias=False), Tanh(),\n",
    "    Linear(100, 27)\n",
    "]\n",
    "parameters_nl = C.parameters() + [p for layer in layers_w_nl for p in layer.parameters()]\n",
    "print(\"Total parameters = \", sum(p.nelement() for p in parameters_nl))\n",
    "\n",
    "# lets make sure that the use grad is true for all parameters.\n",
    "for p in parameters_nl:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Now train like before\n",
    "all_loss = train(x_train, y_train, layers_w_nl, parameters_nl, 100000, C, lr=0.10, verbose=False)\n",
    "print(f'Average last 10 loss: {sum(all_loss[-10:])/10:.2f}')\n",
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248354d",
   "metadata": {},
   "source": [
    "## Log loss summary:\n",
    "Just linear layer: Average 10 loss: 2.60 \n",
    "\n",
    "Added non-linear layer: Average last 10 loss: 1.23 \n",
    "\n",
    "Scaled initialization weights, for good initialization: Average last 10 loss: 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5919795",
   "metadata": {},
   "source": [
    "### Nope. Didn't work.\n",
    "1) Let's see visually how our network is performing.\n",
    "2) Let's split train and validation set.\n",
    "3) train on the training set, and eval loss on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d036806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the histogram of activation\n",
    "def plot_tanh_activation(layers):\n",
    "    plt.figure()\n",
    "    legends = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        if isinstance(layer, Tanh):\n",
    "            t = layer.out\n",
    "            print(\"layer: %d -> %5s: mean %+.2f, std %+.2f, saturated: %.2f\" % (i, layer.__class__.__name__, t.mean().item(), t.std().item(), (t.abs() > 0.97).float().mean() * 100))\n",
    "            hy, hx = torch.histogram(t, density=True)\n",
    "            plt.plot(hx[:-1].detach(), hy.detach())\n",
    "            legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "    plt.legend(legends)\n",
    "    plt.title('activation distribution')\n",
    "# plot_tanh_activation(layers_w_nl)\n",
    "\n",
    "# looks very saturated (most of the neurons are not active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de343e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(layers_w_nl[7].out.abs() > 0.97, cmap='gray')\n",
    "plt.title(\"Activation of layer 7\")\n",
    "# There are a lot of white, i.e., those neurons are not active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5087a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the histogram of gradient\n",
    "def plot_layer_gradient(layers):\n",
    "    plt.figure()\n",
    "    legends = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        if isinstance(layer, Tanh):\n",
    "            t = layer.out.grad\n",
    "            print(\"layer: %d -> %5s: mean %+e, std %+e\" % (i, layer.__class__.__name__, t.mean().item(), t.std().item()))\n",
    "            hy, hx = torch.histogram(t, density=True)\n",
    "            plt.plot(hx[:-1].detach(), hy.detach())\n",
    "            legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "    plt.legend(legends)\n",
    "    plt.title('gradient distribution')\n",
    "\n",
    "# plot_layer_gradient(layers_w_nl)\n",
    "# Gradient looks flattened as the layer increases (not a good sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b1202",
   "metadata": {},
   "source": [
    "# Precise weight tuning is hectic, normalization should be done\n",
    "\n",
    "We want hidden layer activations to be centered around 0 and have a standard deviation of 1, because the gradient is more stable.  Input to Tanh should not be 1 or -1 (i.e., derivative = 0 and the gradient wont flow). \n",
    "\n",
    "So we want to standardize the pre-activation outputs of each layer. \n",
    "We do Batch Normalization for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d491ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)\n",
    "C = Embedding(vocab_size=vocab_size, n_embed=n_embed)\n",
    "layers_w_nl_w_bn = [\n",
    "    Linear(n_embed * block_size, 100, bias=False), BatchNorm(100), Tanh(),\n",
    "    Linear(100, 100, bias=False), BatchNorm(100), Tanh(),\n",
    "    Linear(100, 100, bias=False), BatchNorm(100), Tanh(),\n",
    "    Linear(100, 100, bias=False), BatchNorm(100), Tanh(),\n",
    "    Linear(100, 100, bias=False), BatchNorm(100), Tanh(),\n",
    "    Linear(100, vocab_size), BatchNorm(vocab_size)\n",
    "]\n",
    "parameters_nl = C.parameters() + [p for layer in layers_w_nl_w_bn for p in layer.parameters()]\n",
    "print(\"Total parameters = \", sum(p.nelement() for p in parameters_nl))\n",
    "\n",
    "# lets make sure that the use grad is true for all parameters.\n",
    "for p in parameters_nl:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Now train like before\n",
    "all_loss = train(x_train, y_train, layers_w_nl_w_bn, parameters_nl, 100000, C, lr=0.10, verbose=False)\n",
    "print(f'Average last 10 loss: {sum(all_loss[-10:])/10:.2f}')\n",
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe45480",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tanh_activation(layers_w_nl_w_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851707c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_gradient(layers_w_nl_w_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(1024)\n",
    "@torch.no_grad()\n",
    "def predict(layers, examples=10):\n",
    "    for layer in layers:\n",
    "        layer.training = False\n",
    "    for _ in range(examples):\n",
    "        context = [0] * block_size\n",
    "        output = []\n",
    "        while True:\n",
    "            emb = C(torch.tensor([context]))\n",
    "            x_input = emb.view(1, -1)\n",
    "            for layer in layers:\n",
    "                x_input = layer(x_input)\n",
    "            probs = torch.softmax(x_input, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=generator).item()\n",
    "            context = context[1:] + [ix]\n",
    "            output.append(ix)\n",
    "            if ix == 0:\n",
    "                break\n",
    "        print(''.join(itoc[i] for i in output))\n",
    "\n",
    "\n",
    "predict(layers_w_nl_w_bn, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c1a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
