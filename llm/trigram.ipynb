{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8e603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e154852",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/names.txt', 'r') as f:\n",
    "    data = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ef61d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "228146\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(''.join([d for d in data])))\n",
    "chars = ['.'] + chars\n",
    "ctoi = {c:i for i, c in enumerate(chars)}\n",
    "itoc = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "print(ctoi)\n",
    "print(itoc)\n",
    "X = []\n",
    "block_size = 2 # Tri-gram\n",
    "for word in data:\n",
    "    context = [0] * block_size\n",
    "    for c in word + '.':\n",
    "        ix = ctoi[c]\n",
    "        X.append(context + [ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c33ed240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we can't pass int64 (long) into a neural network, we use embeddings\n",
    "# Here, make embedding for every single characters a-z (26) + '.' (1); Here we define a random number\n",
    "torch.manual_seed(1014)\n",
    "emb = torch.rand(27, 5)\n",
    "# emb[0] is the embedding of '.', emb[1] is for 'a', and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9ca0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0., 4410., 1306., 1542., 1690., 1531.,  417.,  669.,  874.,  591.,\n",
      "        2422., 2963., 1572., 2538., 1146.,  394.,  515.,   92., 1639., 2055.,\n",
      "        1308.,   78.,  376.,  307.,  134.,  535.,  929.])\n"
     ]
    }
   ],
   "source": [
    "# First let's 'train' a simple 'network': Count how many times one character follows another and\n",
    "# use that to sample next word to generate word. NOT using a Neural Network\n",
    "\n",
    "\n",
    "counts = torch.zeros(27, 27, 27)\n",
    "# print(counts.size())\n",
    "# counts[0, 0, 0] represents count of '...' sequence in our training data, counts[0, 1, 0] represents count of '.a.' in our training data and so on\n",
    "for val in X:\n",
    "    counts[val[0], val[1], val[2]] += 1\n",
    "\n",
    "\n",
    "print(counts[0, 0, :])\n",
    "# Now Normalize\n",
    "# '...', '..a', '..b', '..c', '..d', ..... and so on should sum to 1.0 (i.e., normalize across count[0, 0, i])\n",
    "# '.a.', '.aa', '.ab', '.ac', '.ad', ... and so on should sum to 1.0 (i.e., across count[0, 1, i])\n",
    "# '.b.', '.ba', '.bb', '.bc', '.bd', ... and so on should sum to 1.0 (i.e., across count[0, 2, i])\n",
    "# ...\n",
    "# 'z..', 'z.a', 'z.b', 'z.c', 'z.d', ...\n",
    "# ...\n",
    "# 'zz.', 'zza', 'zzb', 'zzc', 'zzd', ...\n",
    "\n",
    "# i.e., for every previous occurrences, we have to sum along dimension = 2 (position of i above)\n",
    "\n",
    "counts = counts + 1 # Avoid division by zero (smoothing) as this 1 increase to higher number, we get uniform distribution. Hence smoothing.\n",
    "counts = counts / counts.sum(dim=2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63034c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Check if it sums to 1.0\n",
    "print(sum(counts[0, 0, :]))\n",
    "print(sum(counts[0, 1, :]))\n",
    "print(sum(counts[0, 1, :]))\n",
    "\n",
    "# Rough code to see dimension\n",
    "# ten = torch.tensor([[[1,2,3],[1,2,3],[1,2,3]], [[10,20,30],[10,20,30],[10,20,30]], [[100,200,300],[100,200,300],[100,200,300]]])\n",
    "# print(ten)\n",
    "\n",
    "# print(\"Sum\")\n",
    "# ten.sum(dim=2, keepdim=True)\n",
    "# dim0 = (3, 3)\n",
    "# dim1 = (1, 3)\n",
    "# dim2 = (3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecef7093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kynn.\n",
      "karthordyn.\n",
      "ja.\n",
      "xen.\n",
      "conn.\n",
      "zoemadiah.\n",
      "lanunne.\n",
      "alen.\n",
      "zeh.\n",
      "saia.\n",
      "jayton.\n",
      "ta.\n",
      "getsmya.\n",
      "ja.\n",
      "azdey.\n",
      "ah.\n",
      "adasenicelyntjrthad.\n",
      "estorraleagra.\n",
      "brafines.\n",
      "johila.\n",
      "nehori.\n",
      "nessmdkken.\n",
      "hanobdincina.\n",
      "kyanaslggxyanaizlyn.\n",
      "non.\n",
      "stytosper.\n",
      "haleikashada.\n",
      "aizliand.\n",
      "olce.\n",
      "aravori.\n",
      "arie.\n",
      "tygtvin.\n",
      "foko.\n",
      "ne.\n",
      "dalyn.\n",
      "ronni.\n",
      "suha.\n",
      "keightya.\n",
      "mar.\n",
      "ri.\n",
      "hen.\n",
      "jell.\n",
      "bree.\n",
      "yunikyrein.\n",
      "yic.\n",
      "mirrhyrivikazenton.\n",
      "lesynn.\n",
      "el.\n",
      "muna.\n",
      "otanew.\n"
     ]
    }
   ],
   "source": [
    "# Now sample from that counts\n",
    "\n",
    "g = torch.Generator().manual_seed(1024)\n",
    "for _ in range(50):\n",
    "    out = []\n",
    "    ix, prev_ix = 0, 0\n",
    "    while True:\n",
    "        # Now sample index from count[0, 0, :] row.\n",
    "        p = counts[prev_ix, ix, :]\n",
    "        new_ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itoc[new_ix])\n",
    "        if new_ix == 0:\n",
    "            break\n",
    "        prev_ix = ix\n",
    "        ix = new_ix\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170e2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
